# Linear_Regression
Linear Regression algorithm for machine learning

Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.

Linear regression has many practical uses. Most applications fall into one of the following two broad categories:

If the goal is prediction, or forecasting, or error reduction,linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.
If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.

Regression Evaluation Metrics
Here are three common evaluation metrics for regression problems:

Mean Absolute Error (MAE) is the mean of the absolute value of the errors:

1ğ‘›âˆ‘ğ‘–=1/ğ‘›|ğ‘¦ğ‘–âˆ’ğ‘¦Ì‚ ğ‘–|
 
Mean Squared Error (MSE) is the mean of the squared errors:

1ğ‘›âˆ‘ğ‘–=1/ğ‘›(ğ‘¦ğ‘–âˆ’ğ‘¦Ì‚ ğ‘–)2
 
Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:

1ğ‘›âˆ‘ğ‘–=(1/ğ‘›(ğ‘¦ğ‘–âˆ’ğ‘¦Ì‚ ğ‘–)^2)^1/2
 
Comparing these metrics:

MAE is the easiest to understand, because it's the average error.
MSE is more popular than MAE, because MSE "punishes" larger errors, which tends to be useful in the real world.
RMSE is even more popular than MSE, because RMSE is interpretable in the "y" units.
All of these are loss functions, because we want to minimize them.
